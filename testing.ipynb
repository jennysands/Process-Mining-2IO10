{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "global uploaded\n",
    "uploaded = \"bpi2012.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility.run\n",
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "from keras import backend\n",
    "import csv\n",
    "import unicodecsv\n",
    "import numpy as np\n",
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "# from tensorflow import keras \n",
    "\n",
    "# from utility.enums import DataGenerationPattern, Processor, RnnType\n",
    "\n",
    "# Dataoperation\n",
    "# from .enums import DataType as dt\n",
    "# from .enums import DataClass as dc\n",
    "# from .enums import FeatureType as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from utility.enums import Processor\n",
    "\n",
    "# def Configure(args):    \n",
    "#     import keras as keras\n",
    "#     print(\"keras version: \" + keras.__version__)\n",
    "\n",
    "#     backendname = backend.backend()\n",
    "#     if backendname == 'cntk':\n",
    "#         __configure_CNTK(args)\n",
    "#     elif backendname == 'tensorflow':\n",
    "#         __configure_tensorflow(args)\n",
    "#     elif backendname == 'theano':\n",
    "#         __configure_Theano(args)\n",
    "#     else:\n",
    "#         raise ValueError(\"unable to detect backend for configuration\")\n",
    "\n",
    "# # def Clean_Session():\n",
    "# #     backendname = backend.backend()\n",
    "# #     if backendname == 'cntk':\n",
    "# #         __clean_session_cntk()\n",
    "# #     elif backendname == 'tensorflow':\n",
    "# #         __clean_session_tensorflow()\n",
    "# #     elif backendname == 'theano':\n",
    "# #         __clean_session_theano()\n",
    "# #     else:\n",
    "# #         raise ValueError(\"unable to detect backend for cleanup\")\n",
    "# #     # cleanup\n",
    "# #     from keras import backend as K\n",
    "# #     K.clear_session()\n",
    "\n",
    "# # def __configure_Theano(args):\n",
    "# #     print(\"theano configured\")\n",
    "\n",
    "# # def __configure_CNTK(args):\n",
    "# #     print(\"cntk configured\")\n",
    "\n",
    "# # def __configure_tensorflow(args):\n",
    "# #     import tensorflow as tf\n",
    "# #     print(\"tensorflow version: \" + tf.__version__)\n",
    "# #     # tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# #     # commented out\n",
    "# #     Clean_Session()\n",
    "\n",
    "#     if(args['processor'] != Processor.GPU):\n",
    "#         import os\n",
    "#         os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "#     if(args['processor'] == Processor.TPU):\n",
    "#         # setup tpu worker for google colab (colab.research.google.com)\n",
    "#         tpu_addr = os.environ.get('COLAB_TPU_ADDR')\n",
    "#         if(tpu_addr is not None):\n",
    "#             args['TPU_WORKER'] = 'grpc://' + tpu_addr\n",
    "#             print(\"tpu worker: {}\".format(args['TPU_WORKER']))\n",
    "#         else:\n",
    "#             args['processor'] = Processor.CPU\n",
    "#             print(\"no tpu worker found, falling back to cpu\")\n",
    "#     elif(args['processor'] == Processor.GPU):\n",
    "# #         from keras.backend.tensorflow_backend import set_session\n",
    "#         config = tf.ConfigProto()\n",
    "#         config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "#         sess = tf.Session(config=config)\n",
    "#         set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "#         print(\"multi process gpu usage: enabled\")\n",
    "#     print(\"tensorflow configured\")\n",
    "\n",
    "# # def __clean_session_theano():\n",
    "# #     print(\"theano session cleaned\")\n",
    "\n",
    "# # def __clean_session_cntk():\n",
    "# #     print(\"cntk session cleaned\")\n",
    "\n",
    "# # def __clean_session_tensorflow():\n",
    "# #     import tensorflow as tf\n",
    "# # #     tf.keras.backend.clear_session()\n",
    "# #     print(\"tensorflow session cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataoperations\n",
    "\n",
    "\n",
    "minutes_in_day = 24 * 60\n",
    "days_in_week = 7\n",
    "\n",
    "def ReadInData(file, row_structure):\n",
    "    csvfile = open(file, 'r')\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(spamreader, None)  # skip the headers\n",
    "    lastcase = ''\n",
    "    firstLine = True\n",
    "    totalrows = 0\n",
    "\n",
    "    #create return array with size of len(row_structure) (columns)\n",
    "    data = []\n",
    "    intermediateData = []\n",
    "    for i in range(len(row_structure)):\n",
    "        data.append([])\n",
    "        intermediateData.append([])\n",
    "\n",
    "    for row in spamreader:\n",
    "        totalrows = totalrows + 1\n",
    "        if row[0] != lastcase:\n",
    "            lastcase = row[0]\n",
    "            if not firstLine:\n",
    "                for i in range(len(row_structure)):\n",
    "                    data[i].append(intermediateData[i])\n",
    "            for i in range(len(row_structure)):\n",
    "                intermediateData[i] = []\n",
    "        for i in range(len(row_structure)):\n",
    "            #cast by defined type\n",
    "            if row_structure[i]['datatype'] == DataType.String:                \n",
    "                intermediateData[i].append(row[row_structure[i]['columnindex']])\n",
    "            elif row_structure[i]['datatype'] == DataType.Float:\n",
    "                intermediateData[i].append(float(row[row_structure[i]['columnindex']]))\n",
    "            elif row_structure[i]['datatype'] == DataType.Int:\n",
    "                intermediateData[i].append(int(row[row_structure[i]['columnindex']]))\n",
    "            else:\n",
    "                raise ValueError(\"unknown datatype encountered\")\n",
    "        firstLine = False\n",
    "    # add last case\n",
    "    for i in range(len(row_structure)):\n",
    "        data[i].append(intermediateData[i])\n",
    "    print(\"read {} rows\".format(totalrows))\n",
    "    return data\n",
    "\n",
    "def VerifyDatadefinition(datadefinition):\n",
    "    # set default values\n",
    "    for index, definition in enumerate(datadefinition):\n",
    "        if 'datatype' not in definition:\n",
    "            definition['datatype'] = DataType.String\n",
    "            print(\"no value found for datadefinition.datatype at index {}, setting default value\".format(index))\n",
    "        if 'dataclass' not in definition:\n",
    "            definition['dataclass'] = DataClass.Onehot\n",
    "            print(\"no value found for datadefinition.dataclass at index {}, setting default value\".format(index))\n",
    "        if 'featuretype' not in definition:\n",
    "            definition['featuretype'] = FeatureType.none\n",
    "            print(\"no value found for datadefinition.featuretype at index {}, setting default value\".format(index))\n",
    "        if 'featureweight' not in definition:\n",
    "            definition['featureweight'] = 1.0\n",
    "            print(\"no value found for datadefinition.featureweight at index {}, setting default value\".format(index))\n",
    "        if 'columnindex' not in definition:\n",
    "            raise ValueError(\"columnindex cannot be empty\")\n",
    "\n",
    "def CalculateFeatures(args):\n",
    "    catvectorlen = 0\n",
    "    num_features = 1 # default 1 because indexing variable\n",
    "    args['feature_weights'] = []\n",
    "    for index, definition in enumerate(args['rowstructure']):\n",
    "        # append feature weight, if non provide add default\n",
    "        if definition['featureweight']:\n",
    "            args['feature_weights'].append(definition['featureweight'])\n",
    "        else:\n",
    "            args['feature_weights'].append(1.0)\n",
    "\n",
    "        if definition['featuretype'] == FeatureType.Train:            \n",
    "            if definition['datatype'] == DataType.String:\n",
    "                if definition['dataclass'] == DataClass.Onehot:\n",
    "                    catvectorlen += len(args['indices']['chars_indices'][index]) # onehot increases vector length by amount of classes (encoded as strings)\n",
    "                elif definition['dataclass'] == DataClass.Multilabel:\n",
    "                    catvectorlen += len(args['indices']['unique_chars_indices'][index]) # ml increases vector length by amount of classes (encoded as chars)\n",
    "            elif definition['datatype'] == DataType.Float:\n",
    "                if definition['dataclass'] == DataClass.Periodic:\n",
    "                    num_features += 2 # periodic values increase by 2 (sin/cos)\n",
    "                else:\n",
    "                    num_features += 1 # every numeric value increases by 1\n",
    "            elif definition['datatype'] == DataType.Int:\n",
    "                if definition['dataclass'] == DataClass.Periodic:\n",
    "                    num_features += 2 # periodic values increase by 2 (sin/cos)\n",
    "                else:\n",
    "                    num_features += 1 # every numeric value increases by 1            \n",
    "\n",
    "    args['catvectorlen'] = catvectorlen\n",
    "    print('category vectors length:', args['catvectorlen'])\n",
    "    args['num_features'] = catvectorlen + num_features\n",
    "    print('num features: {}'.format(args['num_features']))\n",
    "    return\n",
    "\n",
    "def AppendEOL(data):\n",
    "    outdata = []\n",
    "    for i in range(len(data)):\n",
    "        if isinstance(data[i][0], float) or isinstance(data[i][0], int):\n",
    "            outdata.append(list(map(lambda x: x + [0], data[i])))\n",
    "        else:\n",
    "            outdata.append(list(map(lambda x: x + ['!'],data[i])))\n",
    "    return outdata\n",
    "\n",
    "def TruncateSequences(indata, maxlength):\n",
    "    outdata = []\n",
    "    for i in range(len(indata)):\n",
    "        outdata.append([])\n",
    "    for i in range(len(indata[0])):\n",
    "        if(len(indata[0][i]) <= maxlength):\n",
    "            for j in range(len(indata)):\n",
    "                outdata[j].append(indata[j][i])\n",
    "    print(\"sequences truncated to {}\".format(maxlength))\n",
    "    return outdata\n",
    "\n",
    "def CreateDivisors(data):\n",
    "    # get divisors for normalization\n",
    "    divisors = []\n",
    "    for i in range(len(data)):\n",
    "        if isinstance(data[i][0][0], float) or isinstance(data[i][0][0], int):\n",
    "            divisors.append(np.mean([item for sublist in data[i] for item in sublist]))\n",
    "        else:\n",
    "            divisors.append(\"null\")\n",
    "        print('divisor{0}: {1}'.format(i, divisors[i]))\n",
    "    return divisors\n",
    "\n",
    "def CreateOffsets(data):\n",
    "    offsets = []\n",
    "    for i in range(len(data)):\n",
    "        if isinstance(data[i][0][0], float) or isinstance(data[i][0][0], int):\n",
    "            # normalize to: 0 to x\n",
    "            offset = np.min([item for sublist in data[i] for item in sublist])\n",
    "            if offset < 0:\n",
    "                datatemp = []\n",
    "                for seq in data[i]:\n",
    "                    datatempint = []\n",
    "                    for elem in seq:\n",
    "                        datatempint.append(elem - offset)\n",
    "                    datatemp.append(datatempint)\n",
    "                data[i] = datatemp\n",
    "                offsets.append(offset)\n",
    "            else:\n",
    "                offsets.append(0)\n",
    "        else:\n",
    "            offsets.append(\"null\")\n",
    "        print('offset{0}: {1}'.format(i, offsets[i]))\n",
    "    return offsets\n",
    "\n",
    "def CreateDictionaries(data, rowstructure):    \n",
    "    resultDict = {\n",
    "        \"chars\" : [], # corpus\n",
    "        \"unique_chars\" : [], # corpus elements from multilabel classification\n",
    "        \"target_chars\" : [], # corpus including eol\n",
    "        \"target_unique_chars\" : [], # corpus elements from multilabel classification including eol\n",
    "\n",
    "        \"chars_indices\" : [], # chars - index mapping\n",
    "        \"unique_chars_indices\" : [], # unique chars - index mapping\n",
    "        \"target_chars_indices\" : [], # target chars - index mapping\n",
    "        \"target_unique_chars_indices\" : [],\n",
    "\n",
    "        \"indices_chars\" : [], # index - chars mapping\n",
    "        \"indices_target_chars\" : [], # index - target chars mapping\n",
    "        \"indices_unique_chars\" : [], # index - unique chars mapping\n",
    "        \"indices_target_unique_chars\" : []\n",
    "    }\n",
    "\n",
    "    for i in range(len(data)): \n",
    "        if isinstance(data[i][0][0], str) and (rowstructure[i]['dataclass'] == DataClass.Onehot or rowstructure[i]['dataclass'] == DataClass.Multilabel):\n",
    "            # get chars\n",
    "            buffer = map(lambda x : set(x),data[i])\n",
    "            buffer = list(set().union(*buffer))\n",
    "            buffer.sort()\n",
    "            resultDict[\"target_chars\"].append(copy.copy(buffer)) # fill target_chars\n",
    "            buffer.remove('!')\n",
    "            resultDict[\"chars\"].append(buffer) # fill chars\n",
    "            print('total chars: {}, target chars: {}'.format(len(resultDict[\"chars\"][i]), len(resultDict[\"target_chars\"][i])))\n",
    "            print('characters: ', resultDict[\"chars\"][i])\n",
    "\n",
    "            #get unique chars\n",
    "            buffer = [l for word in resultDict[\"chars\"][i] for l in word]\n",
    "            buffer.append('!')\n",
    "            buffer = list(set(buffer))\n",
    "            buffer.sort()\n",
    "            resultDict[\"target_unique_chars\"].append(copy.copy(buffer)) # fill target_unique_chars\n",
    "            buffer.remove('!')\n",
    "            resultDict[\"unique_chars\"].append(buffer) # fill unique_chars\n",
    "            print('unique characters: ', resultDict[\"unique_chars\"][i])\n",
    "\n",
    "            # map index dictionaries\n",
    "            resultDict[\"chars_indices\"].append(dict((c, i) for i, c in enumerate(resultDict[\"chars\"][i]))) #dictionary<key,value> with <char, index> where char is unique symbol for activity\n",
    "            resultDict[\"unique_chars_indices\"].append(dict((c, i) for i, c in enumerate(resultDict[\"unique_chars\"][i])))\n",
    "            resultDict[\"target_chars_indices\"].append(dict((c, i) for i, c in enumerate(resultDict[\"target_chars\"][i])))\n",
    "            resultDict[\"target_unique_chars_indices\"].append(dict((c, i) for i, c in enumerate(resultDict[\"target_unique_chars\"][i])))\n",
    "            resultDict[\"indices_chars\"].append(dict((i, c) for i, c in enumerate(resultDict[\"chars\"][i]))) #dictionary<key,value> with <index, char> where char is unique symbol for activity\n",
    "            resultDict[\"indices_unique_chars\"].append( dict((i, c) for i, c in enumerate(resultDict[\"unique_chars\"][i])))\n",
    "            resultDict[\"indices_target_chars\"].append(dict((i, c) for i, c in enumerate(resultDict[\"target_chars\"][i])))\n",
    "            resultDict[\"indices_target_unique_chars\"].append(dict((i, c) for i, c in enumerate(resultDict[\"target_unique_chars\"][i])))\n",
    "        else: # append empty dictionaries for non-string types\n",
    "            resultDict[\"target_chars\"].append([])\n",
    "            resultDict[\"chars\"].append([])\n",
    "            resultDict[\"target_unique_chars\"].append([])\n",
    "            resultDict[\"unique_chars\"].append([])\n",
    "            resultDict[\"chars_indices\"].append([]) \n",
    "            resultDict[\"unique_chars_indices\"].append([])\n",
    "            resultDict[\"target_chars_indices\"].append([])\n",
    "            resultDict[\"target_unique_chars_indices\"].append([])\n",
    "            resultDict[\"indices_chars\"].append([])\n",
    "            resultDict[\"indices_unique_chars\"].append([])\n",
    "            resultDict[\"indices_target_chars\"].append([])\n",
    "            resultDict[\"indices_target_unique_chars\"].append([])\n",
    "    return resultDict\n",
    "\n",
    "def CreateSentences(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        buffer = []\n",
    "        for j in range(len(data[0])):\n",
    "            for k in range(len(data[0][j])):\n",
    "                if k == 0: # skip sequence with length = 1\n",
    "                    continue\n",
    "                buffer.append(data[i][j][0:k])\n",
    "        sentences.append(buffer)\n",
    "    return sentences\n",
    "\n",
    "def CreateNgramsFromLabels(data, rowstructure, ngram_size):\n",
    "    \"\"\" replaces string input labels with their ngram representation \"\"\"\n",
    "\n",
    "    if type(ngram_size) is not int:\n",
    "        raise ValueError(\"ngram_size is not an integer\")\n",
    "    if ngram_size < 1:\n",
    "        raise ValueError(\"ngram_size must be an integer >= 1\")\n",
    "\n",
    "    for i, column in enumerate(rowstructure):\n",
    "        #only ngram string variables\n",
    "        if column['datatype'] == DataType.String:\n",
    "            #iterate through data\n",
    "            for j in range(len(data[i])):\n",
    "                # iterate through words in sentence\n",
    "                newsentence = []\n",
    "                for k in range(len(data[i][j])):\n",
    "                    newword = []\n",
    "                    for l in range(ngram_size - 1,-1,-1):\n",
    "                        if  k-l >= 0:\n",
    "                            newword.append(data[i][j][k-l])\n",
    "                    newsentence.append(' '.join(newword))\n",
    "                data[i][j] = newsentence\n",
    "            print(\"created ngrams for column \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enums.py\n",
    "class DataType(Enum):\n",
    "    String = \"string\"\n",
    "    Float = \"float\"\n",
    "    Int = \"int\"\n",
    "\n",
    "class DataClass(Enum):\n",
    "    Onehot = \"onehot\"\n",
    "    Multilabel = \"multilabel\"\n",
    "    Numeric = \"numeric\"\n",
    "    Periodic = \"periodic\"\n",
    "    Integer = \"integer\"\n",
    "    none = \"none\"\n",
    "\n",
    "class RnnType(Enum):\n",
    "    RNN = \"rnn\"\n",
    "    LSTM = \"lstm\"\n",
    "    GRU = \"gru\"\n",
    "\n",
    "class Processor(Enum):\n",
    "    CPU = \"cpu\"\n",
    "    GPU = \"gpu\"\n",
    "    TPU = \"tpu\"\n",
    "\n",
    "class FeatureType(Enum):\n",
    "    Train = \"train\"\n",
    "    Target = \"target\"\n",
    "    none = \"none\"\n",
    "\n",
    "class DataGenerationPattern(Enum):\n",
    "    Fit = \"Fit\" # classic approach (aka load all into memory)\n",
    "    Generator = \"Generator\" # generator approach (e.g. fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptions.py\n",
    "   \n",
    "class Error(Exception):\n",
    "    pass\n",
    "\n",
    "class ConductorError(Error):\n",
    "    \"\"\"Exception raised for errors during execution. Handled via custom element to make parsing easier\n",
    "    Attributes:\n",
    "        message -- explanation of the error\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator.py\n",
    "\n",
    "#TODO: make tpu compliant\n",
    "#TODO: shuffle switch\n",
    "\n",
    "def count_sentences(data):\n",
    "    datasize = 0\n",
    "    for i in range(0,len(data[0])):\n",
    "        datasize += len(data[0][i]) - 1 # remove EOL character\n",
    "    print(\"count generator with {} sequences and {} sentences\".format(len(data[0]), datasize))\n",
    "    return datasize\n",
    "\n",
    "def GenerateGenerator(isTensorflow, data, args,shuffle=True):\n",
    "    # factory function\n",
    "    if isTensorflow == True:\n",
    "        import tensorflow.keras as keras\n",
    "    else:\n",
    "        import keras as keras\n",
    "    generator = __GenerateGenerator(keras.utils.Sequence, data, args, shuffle)\n",
    "    return generator\n",
    "\n",
    "def __GenerateGenerator(base, data, args, shuffle):\n",
    "    class DataGenerator(base):\n",
    "            'Generates data for Keras'\n",
    "            def __init__(self, data, args, shuffle):\n",
    "                'Initialization'\n",
    "                self.data = data\n",
    "                self.shuffle = shuffle\n",
    "                self.datadefinition = args['datadefinition']\n",
    "                self.args = args\n",
    "                self.datasize = 0\n",
    "                self.current_index = 0\n",
    "\n",
    "                self.buffer = []\n",
    "                self.outbuffer = []\n",
    "                self.newbuffer = []\n",
    "\n",
    "                #self.on_epoch_end()\n",
    "                #self.__shuffle_data()\n",
    "\n",
    "                for i in range(len(data)):\n",
    "                    self.buffer.append([])\n",
    "\n",
    "                for i in range(0,len(self.data[0])):\n",
    "                    self.datasize += len(self.data[0][i]) - 1 # remove EOL character\n",
    "                print(\"created generator with with {} sequences and {} sentences\".format(len(self.data[0]), self.datasize))\n",
    "\n",
    "                if self.shuffle:\n",
    "                    self.__shuffle_data()\n",
    "\n",
    "            def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.floor(self.datasize / self.args['batch_size']))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # fill buffer until threshold reached\n",
    "                while len(self.buffer[0]) < self.args['batch_size']:\n",
    "                    sequence = []\n",
    "                    for i in range(len(self.data)):\n",
    "                        sequence.append([])\n",
    "                        sequence[i].append(self.data[i][self.current_index])      \n",
    "                    sentences = CreateSentences(sequence)\n",
    "                    self.current_index +=1 \n",
    "                    if self.current_index >= len(data[0]):\n",
    "                        self.current_index = 0\n",
    "                    # put sentences into buffer\n",
    "                    for i in range(len(self.data)):\n",
    "                        for j in range(len(sentences[i])):\n",
    "                            self.buffer[i].append(sentences[i][j])\n",
    "                \n",
    "                # if buffer has reached certain size, yield\n",
    "                self.outbuffer = []\n",
    "                self.newbuffer = []\n",
    "                for i in range(len(self.buffer)):\n",
    "                    self.outbuffer.append([])\n",
    "                    self.newbuffer.append([])\n",
    "                    for j in range(len(self.buffer[i])):\n",
    "                        if j < self.args['batch_size']:\n",
    "                            self.outbuffer[i].append(self.buffer[i][j])\n",
    "                        else:\n",
    "                            self.newbuffer[i].append(self.buffer[i][j])\n",
    "                # remove used entries by setting a new buffer with remaining entries\n",
    "                self.buffer = self.newbuffer\n",
    "                matrix = self.datadefinition.CreateMatrices(self.outbuffer,self.args)\n",
    "\n",
    "                return matrix['X'], matrix['y_t']        \n",
    "\n",
    "            def on_epoch_end(self):\n",
    "                'after each epoch: reset index and shuffle'\n",
    "                self.current_index = 0\n",
    "                if self.shuffle == True:\n",
    "                    self.__shuffle_data()\n",
    "\n",
    "            def __shuffle_data(self):\n",
    "                self.data = ShuffleArray(self.data)\n",
    "    return DataGenerator(data, args, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "# from utility.dataoperations import CreateSentences\n",
    "# from utility.regularization import ShuffleArray\n",
    "\n",
    "#TODO: make tpu compliant\n",
    "#TODO: shuffle switch\n",
    "\n",
    "def count_sentences(data):\n",
    "    datasize = 0\n",
    "    for i in range(0,len(data[0])):\n",
    "        datasize += len(data[0][i]) - 1 # remove EOL character\n",
    "    print(\"count generator with {} sequences and {} sentences\".format(len(data[0]), datasize))\n",
    "    return datasize\n",
    "\n",
    "def GenerateGenerator(isTensorflow, data, args,shuffle=True):\n",
    "    # factory function\n",
    "    if isTensorflow == True:\n",
    "        import tensorflow.keras as keras\n",
    "    else:\n",
    "        import keras as keras\n",
    "    generator = __GenerateGenerator(keras.utils.Sequence, data, args, shuffle)\n",
    "    return generator\n",
    "\n",
    "def __GenerateGenerator(base, data, args, shuffle):\n",
    "    class DataGenerator(base):\n",
    "            'Generates data for Keras'\n",
    "            def __init__(self, data, args, shuffle):\n",
    "                'Initialization'\n",
    "                self.data = data\n",
    "                self.shuffle = shuffle\n",
    "                self.datadefinition = args['datadefinition']\n",
    "                self.args = args\n",
    "                self.datasize = 0\n",
    "                self.current_index = 0\n",
    "\n",
    "                self.buffer = []\n",
    "                self.outbuffer = []\n",
    "                self.newbuffer = []\n",
    "\n",
    "                #self.on_epoch_end()\n",
    "                #self.__shuffle_data()\n",
    "\n",
    "                for i in range(len(data)):\n",
    "                    self.buffer.append([])\n",
    "\n",
    "                for i in range(0,len(self.data[0])):\n",
    "                    self.datasize += len(self.data[0][i]) - 1 # remove EOL character\n",
    "                print(\"created generator with with {} sequences and {} sentences\".format(len(self.data[0]), self.datasize))\n",
    "\n",
    "                if self.shuffle:\n",
    "                    self.__shuffle_data()\n",
    "\n",
    "            def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.floor(self.datasize / self.args['batch_size']))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # fill buffer until threshold reached\n",
    "                while len(self.buffer[0]) < self.args['batch_size']:\n",
    "                    sequence = []\n",
    "                    for i in range(len(self.data)):\n",
    "                        sequence.append([])\n",
    "                        sequence[i].append(self.data[i][self.current_index])      \n",
    "                    sentences = CreateSentences(sequence)\n",
    "                    self.current_index +=1 \n",
    "                    if self.current_index >= len(data[0]):\n",
    "                        self.current_index = 0\n",
    "                    # put sentences into buffer\n",
    "                    for i in range(len(self.data)):\n",
    "                        for j in range(len(sentences[i])):\n",
    "                            self.buffer[i].append(sentences[i][j])\n",
    "                \n",
    "                # if buffer has reached certain size, yield\n",
    "                self.outbuffer = []\n",
    "                self.newbuffer = []\n",
    "                for i in range(len(self.buffer)):\n",
    "                    self.outbuffer.append([])\n",
    "                    self.newbuffer.append([])\n",
    "                    for j in range(len(self.buffer[i])):\n",
    "                        if j < self.args['batch_size']:\n",
    "                            self.outbuffer[i].append(self.buffer[i][j])\n",
    "                        else:\n",
    "                            self.newbuffer[i].append(self.buffer[i][j])\n",
    "                # remove used entries by setting a new buffer with remaining entries\n",
    "                self.buffer = self.newbuffer\n",
    "                matrix = self.datadefinition.CreateMatrices(self.outbuffer,self.args)\n",
    "\n",
    "                return matrix['X'], matrix['y_t']        \n",
    "\n",
    "            def on_epoch_end(self):\n",
    "                'after each epoch: reset index and shuffle'\n",
    "                self.current_index = 0\n",
    "                if self.shuffle == True:\n",
    "                    self.__shuffle_data()\n",
    "\n",
    "            def __shuffle_data(self):\n",
    "                self.data = ShuffleArray(self.data)\n",
    "    return DataGenerator(data, args, shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "def CreateModel(args):\n",
    "    keras_impl = __getKerasImplementation(args['processor'])\n",
    "    if(args['processor'] == Processor.TPU):\n",
    "        print(\"imported tensorflow.keras API for model creation\")\n",
    "    else:\n",
    "        print(\"imported keras API for model creation\")\n",
    "\n",
    "    if args['rnntype'] == RnnType.LSTM :\n",
    "        if args['processor'] == Processor.GPU and args['cudnn'] == True:\n",
    "            print(\"creating stateless cudnn lstm model\")\n",
    "            return __createCUDNN_LSTM_Stateless(keras_impl,args) \n",
    "        else:\n",
    "            print(\"creating stateless lstm model\")\n",
    "            return __createLSTM_Stateless(keras_impl,args)  \n",
    "    elif args['rnntype'] == RnnType.GRU:\n",
    "        if args['processor'] == Processor.GPU and args['cudnn'] == True:\n",
    "            print(\"creating stateless cudnn gru model\")\n",
    "            return __createCUDNN_GRU_Stateless(keras_impl,args) \n",
    "        else:\n",
    "            print(\"creating stateless gru model\")\n",
    "            return __createGRU_Stateless(keras_impl,args)   \n",
    "    elif args['rnntype'] == RnnType.RNN:\n",
    "        print(\"creating stateless rnn model\")\n",
    "        return __createRNN_Stateless(keras_impl,args) \n",
    "    else:\n",
    "        raise ValueError(\"unkown model type\")\n",
    "\n",
    "def CreateCallbacks(args):\n",
    "    callbacks = []\n",
    "    keras_impl = __getKerasImplementation(args['processor'])\n",
    "\n",
    "    callbacks.append(keras_impl.callbacks.EarlyStopping(monitor='val_loss', patience=args['patience_earlystopping']))\n",
    "    if(args['save_model'] == True):\n",
    "        callbacks.append(keras_impl.callbacks.ModelCheckpoint(args['modelfilename'], monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto'))\n",
    "    if(args['processor'] != Processor.TPU):\n",
    "        callbacks.append(keras_impl.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=args['patience_reducelr'], verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "    callbacks.append(keras_impl.callbacks.CSVLogger('{}_epochlogs.epochlog'.format(args['running'])))\n",
    "    if args['tensorboard'] == True:\n",
    "        callsbacks.append(tensorboard_cb = TensorBoard(log_dir='./graph', histogram_freq=0, write_graph=True, write_images=True))\n",
    "    return callbacks\n",
    "\n",
    "def CreateOptimizer(keras_impl, args):\n",
    "    if args['processor'] == Processor.TPU:\n",
    "        import tensorflow as tf \n",
    "        return tf.contrib.opt.NadamOptimizer(learning_rate=args['learningrate'], beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "    else:\n",
    "        return keras_impl.optimizers.Nadam(lr=args['learningrate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=args['gradientclipvalue'])   \n",
    "\n",
    "def __getKerasImplementation(processor):\n",
    "    if(processor == Processor.TPU):\n",
    "        import tensorflow\n",
    "        from tensorflow.python import keras as keras_impl\n",
    "    else:\n",
    "        import keras as keras_impl\n",
    "    return keras_impl\n",
    "\n",
    "#lstm\n",
    "def __createCUDNN_LSTM_Stateless(keras_impl,args):\n",
    "#     model = keras_impl.models.Sequential()\n",
    "    model = keras_impl.Sequential()\n",
    "    if args['layers'] == 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNLSTM(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform'),input_shape=(args['maxlen'],args['num_features']))) \n",
    "        else:\n",
    "            model.add(keras_impl.layers.CuDNNLSTM(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=False, kernel_initializer='glorot_uniform'))        \n",
    "    if args['layers'] > 1:\n",
    "        if args['bidirectional'] == True:            \n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNLSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform'),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.CuDNNLSTM(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        for i in range(args['layers'] - 1):\n",
    "            model.add(keras_impl.layers.BatchNormalization())\n",
    "            if i == args['layers'] - 2:\n",
    "                if args['bidirectional'] == True:\n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNLSTM(args['neurons'], return_sequences=False, kernel_initializer='glorot_uniform'))) \n",
    "                else:\n",
    "                    model.add(keras_impl.layers.CuDNNLSTM(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform'))                  \n",
    "            else:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNLSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform')))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.CuDNNLSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "    model.add(keras_impl.layers.BatchNormalization())\n",
    "    model.add(keras_impl.layers.Dense(1, kernel_initializer='glorot_uniform', name='time_output'))\n",
    "\n",
    "    opt = CreateOptimizer(keras_impl, args)\n",
    "    model.compile(loss={'time_output':'mae'}, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def __createLSTM_Stateless(keras_impl,args):\n",
    "#     model = keras_impl.models.Sequential()\n",
    "    model = keras_impl.Sequential()\n",
    "\n",
    "    if args['layers'] == 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.LSTM(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.LSTM(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "    if args['layers'] > 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.LSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.LSTM(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "        for i in range(args['layers'] - 1):\n",
    "            model.add(keras_impl.layers.BatchNormalization())\n",
    "            if i == args['layers'] - 2:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.LSTM(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.LSTM(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))                \n",
    "            else:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.LSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.LSTM(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "    model.add(keras_impl.layers.BatchNormalization())\n",
    "    model.add(keras_impl.layers.Dense(1, kernel_initializer='glorot_uniform', name='time_output'))\n",
    "\n",
    "    opt = CreateOptimizer(keras_impl, args)   \n",
    "    model.compile(loss={'time_output':'mae'}, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "#gru\n",
    "def __createCUDNN_GRU_Stateless(keras_impl,args):\n",
    "#     model = keras_impl.models.Sequential()\n",
    "    model = keras_impl.Sequential()\n",
    "\n",
    "\n",
    "    if args['layers'] == 1:\n",
    "        if args['bidirectional'] == True:            \n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform'),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "             model.add(keras_impl.layers.CuDNNGRU(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=False, kernel_initializer='glorot_uniform'))       \n",
    "    if args['layers'] > 1:\n",
    "        if args['bidirectional'] == True:            \n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform'),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.CuDNNGRU(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "        for i in range(args['layers'] - 1):\n",
    "            model.add(keras_impl.layers.BatchNormalization())\n",
    "            if i == args['layers'] - 2:\n",
    "                if args['bidirectional'] == True:            \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform')))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform'))                  \n",
    "            else:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform')))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.CuDNNGRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform'))\n",
    "    model.add(keras_impl.layers.BatchNormalization())\n",
    "    model.add(keras_impl.layers.Dense(1, kernel_initializer='glorot_uniform', name='time_output'))\n",
    "\n",
    "    opt = CreateOptimizer(keras_impl, args)\n",
    "    model.compile(loss={'time_output':'mae'}, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def __createGRU_Stateless(keras_impl,args):\n",
    "#     model = keras_impl.models.Sequential()\n",
    "    model = keras_impl.Sequential()\n",
    "    if args['layers'] == 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.GRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.GRU(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))        \n",
    "    if args['layers'] > 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.GRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.GRU(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "        for i in range(args['layers'] - 1):\n",
    "            model.add(keras_impl.layers.BatchNormalization())\n",
    "            if i == args['layers'] - 2:\n",
    "                if args['bidirectional'] == True:\n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.GRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.GRU(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))                \n",
    "            else:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.GRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.GRU(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "    model.add(keras_impl.layers.BatchNormalization())\n",
    "    model.add(keras_impl.layers.Dense(1, kernel_initializer='glorot_uniform', name='time_output'))\n",
    "\n",
    "    opt = CreateOptimizer(keras_impl, args)\n",
    "    model.compile(loss={'time_output':'mae'}, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "#rnn (vanilla)\n",
    "def __createRNN_Stateless(keras_impl,args):\n",
    "#     model = keras_impl.models.Sequential()\n",
    "    model = keras_impl.Sequential()\n",
    "    if args['layers'] == 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.SimpleRNN(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))        \n",
    "    if args['layers'] > 1:\n",
    "        if args['bidirectional'] == True:\n",
    "            model.add(keras_impl.layers.Bidirectional(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']),input_shape=(args['maxlen'],args['num_features'])))\n",
    "        else:\n",
    "            model.add(keras_impl.layers.SimpleRNN(args['neurons'],input_shape=(args['maxlen'],args['num_features']),return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "        for i in range(args['layers'] - 1):\n",
    "            model.add(keras_impl.layers.BatchNormalization())\n",
    "            if i == args['layers'] - 2:\n",
    "                if args['bidirectional'] == True:\n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=False, kernel_initializer='glorot_uniform', dropout=args['dropout']))                \n",
    "            else:\n",
    "                if args['bidirectional'] == True: \n",
    "                    model.add(keras_impl.layers.Bidirectional(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout'])))\n",
    "                else:\n",
    "                    model.add(keras_impl.layers.SimpleRNN(args['neurons'],return_sequences=True, kernel_initializer='glorot_uniform', dropout=args['dropout']))\n",
    "    model.add(keras_impl.layers.BatchNormalization())\n",
    "    model.add(keras_impl.layers.Dense(1, kernel_initializer='glorot_uniform', name='time_output'))\n",
    "\n",
    "    opt = CreateOptimizer(keras_impl, args)\n",
    "    model.compile(loss={'time_output':'mae'}, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_Args(**kwargs):\n",
    "    outArgs = {}\n",
    "\n",
    "    # default values if not defined\n",
    "    outArgs['eventlog'] = \"\"\n",
    "    outArgs['running'] = 0\n",
    "    outArgs['bagging'] = True\n",
    "    outArgs['bagging_putback'] = True\n",
    "    outArgs['bagging_size'] = 0.8\n",
    "    outArgs['traindata_split'] = 1\n",
    "    outArgs['traindata_index'] = 0\n",
    "    outArgs['traindata_duplicate'] = 0\n",
    "    outArgs['traindata_shuffle'] = False\n",
    "    outArgs['validationdata_split'] = 0.2\n",
    "    outArgs['testdata_split'] = 0.3333\n",
    "    outArgs['max_sequencelength'] = 3000\n",
    "    outArgs['batch_size'] = 16\n",
    "    outArgs['neurons'] = 100\n",
    "    outArgs['layers'] = 2\n",
    "    outArgs['dropout'] = 0.1\n",
    "    outArgs['max_epochs'] = 500\n",
    "    outArgs['learningrate'] = 0.002\n",
    "    outArgs['patience_earlystopping'] = 40\n",
    "    outArgs['patience_reducelr'] = 10\n",
    "    outArgs['gradientclipvalue'] = 3\n",
    "    outArgs['processor'] = Processor.CPU\n",
    "    outArgs['verbose'] = False\n",
    "    outArgs['rnntype'] = RnnType.LSTM\n",
    "    outArgs['bidirectional'] = False\n",
    "    outArgs['cudnn'] = False\n",
    "    outArgs['tensorboard'] = False\n",
    "    outArgs['save_model'] = False\n",
    "    outArgs['datageneration_pattern'] = DataGenerationPattern.Fit\n",
    "    outArgs['target_host'] = ''\n",
    "\n",
    "    # copy in data\n",
    "    for key,value in kwargs.items():\n",
    "        outArgs[key] = value\n",
    "        print('{} defined: {}'.format(key,value)) \n",
    "\n",
    "    # default values for specific switches\n",
    "    if outArgs['processor'] == Processor.TPU:\n",
    "        outArgs['save_model'] = False\n",
    "        print('processor = tpu; setting save_model to false, because model_checkpoint slows training down for tpu due to weight copying')\n",
    "\n",
    "    return outArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization.py\n",
    "\n",
    "\n",
    "def ShuffleArray(array):\n",
    "    random_state = numpy.random.get_state()\n",
    "    intermediate = []\n",
    "    for i in range(len(array)):\n",
    "        numpy.random.shuffle(array[i])\n",
    "        shuffledArray = array[i]\n",
    "        # reset state to before shuffle, to ensure same shuffling result https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "        numpy.random.set_state(random_state) \n",
    "        intermediate.append(shuffledArray)\n",
    "    return intermediate\n",
    "\n",
    "def BagArray(bagging_size, array, putback):\n",
    "    intermediate = []\n",
    "    if putback == False and bagging_size >= 1:\n",
    "        bagging_size = 1\n",
    "    indices = list(range(0, int(round(bagging_size * len(array[0])))))\n",
    "    for i in range(len(array)):\n",
    "        intermediate.append([])\n",
    "    if putback == True:\n",
    "        for i in range(int(round(bagging_size * len(array[0])))):\n",
    "            num = random.randint(0, len(array[0]) - 1)\n",
    "            for j in range(len(array)):\n",
    "                intermediate[j].append(array[j][num])\n",
    "    elif putback == False:\n",
    "        narray = ShuffleArray(array)\n",
    "        for i in indices:\n",
    "            for j in range(len(narray)):\n",
    "                intermediate[j].append(narray[j][i])\n",
    "    return intermediate\n",
    "\n",
    "def DuplicateData(duplicate_size, array):\n",
    "    intermediate = []\n",
    "    # create empty array\n",
    "    for i in range(len(array)):\n",
    "        intermediate.append([])\n",
    "    # copy array\n",
    "    for i in range(len(array[0])):\n",
    "        for j in range(len(array)):\n",
    "            intermediate[j].append(array[j][i])\n",
    "    # add additional duplicates\n",
    "    number_of_dupes = int(round(duplicate_size * len(array[0])))\n",
    "    if number_of_dupes > 0:     \n",
    "        # iterate through entire set (number_of_dupes > 1)\n",
    "        for i in range(int(number_of_dupes / len(array[0]))):\n",
    "            for j in range(len(array[0])):\n",
    "                for k in range(len(array)):\n",
    "                    intermediate[k].append(array[k][j])              \n",
    "        # iterate through part of set (modulo)        \n",
    "        for i in range(number_of_dupes % len(array[0])): \n",
    "            for j in range(len(array)):\n",
    "                intermediate[j].append(array[j][i])\n",
    "\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run.py\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "import unicodecsv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from os.path import basename\n",
    "import copy\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from math import log\n",
    "\n",
    "# custom imports\n",
    "# import utility.dataoperations as dataoperations\n",
    "# import utility.models as models\n",
    "# import utility.regularization as regularization\n",
    "# import utility.preprocessing as preprocessing\n",
    "# import utility.configuration as configuration\n",
    "# import utility.generator as generator\n",
    "# import utility.exceptions as exceptions\n",
    "# import utility.send_results as send_results\n",
    "# from utility.enums import Processor, DataGenerationPattern\n",
    "\n",
    "\n",
    "def Train_And_Evaluate(**kwargs):\n",
    "    try:\n",
    "        # args = preprocessing.Parse_Args(**kwargs)\n",
    "        args = Parse_Args(**kwargs)\n",
    "\n",
    "        # get dataset specific parameters\n",
    "        datadefinition = args['datadefinition']\n",
    "        args['rowstructure'] = datadefinition.GetRowstructure()\n",
    "        if args['eventlog'] == \"\": # get dataset defined in rowstructure if not explicitly supplied\n",
    "            args['eventlog'] = datadefinition.GetDataset()\n",
    "\n",
    "        #setup (e.g. backend specifics)\n",
    "        # configuration.Configure(args)\n",
    "#         Configure(args)\n",
    "        \n",
    "        args = __Preprocessing(args)\n",
    "        model = __Train_Model(args)\n",
    "        __Evaluate_Model(args, model)\n",
    "    except Exception as ex:\n",
    "        # catch all, throw one exception for parsing\n",
    "        # raise exceptions.ConductorError(ex)\n",
    "        raise ConductorError(ex)\n",
    "\n",
    "def __ReadData(indata, args):\n",
    "    #read from csv file\n",
    "#     data = dataoperations.ReadInData(args[indata],args['rowstructure'])\n",
    "    data = ReadInData(args[indata],args['rowstructure'])\n",
    "\n",
    "    # remove sequences longer than maxseqlength\n",
    "#     data = dataoperations.TruncateSequences(data,args['max_sequencelength'])\n",
    "    data = TruncateSequences(data,args['max_sequencelength'])\n",
    "    return data\n",
    "\n",
    "def __Preprocessing(args): \n",
    "    # clean datadefinion\n",
    "#     dataoperations.VerifyDatadefinition(args['rowstructure'])\n",
    "    VerifyDatadefinition(args['rowstructure'])\n",
    "\n",
    "    #define result filenames\n",
    "    args['modelfilename'] = '{}_model.h5'.format(args['running'])\n",
    "    args['testresultsfilename'] = '{}_results.csv'.format(args['running'])\n",
    "\n",
    "    # read from csv file\n",
    "    data = __ReadData(\"eventlog\",args)\n",
    "\n",
    "    # offset data (if < 0) and create divisors\n",
    "    args['offsets'] = CreateOffsets(data)\n",
    "    args['divisors'] = CreateDivisors(data)\n",
    "\n",
    "    # append EOL characters\n",
    "    data = AppendEOL(data)\n",
    "    args['maxlen'] = max(map(lambda x: len(x),data[0])) - 1 # minus eol\n",
    "    print('maxlen {} '.format(args['maxlen']))\n",
    "\n",
    "    # extract corpus\n",
    "    args['indices'] = CreateDictionaries(data,args['rowstructure'])\n",
    "\n",
    "    # calc feature lengths\n",
    "    CalculateFeatures(args)\n",
    "\n",
    "    # generate folds\n",
    "    intermediate_fold_data = []\n",
    "    args['traindata'] = []\n",
    "    args['validationdata'] = []\n",
    "    args['testdata'] = []\n",
    "    # first round of folds\n",
    "    split_index = int(len(data[0]) - (len(data[0]) * args['testdata_split']))\n",
    "    for i in range(len(data)):\n",
    "        intermediate_fold_data.append(data[i][:split_index])\n",
    "        args['testdata'].append(data[i][split_index:])\n",
    "    # second round of folds\n",
    "    split_index = int(len(intermediate_fold_data[0]) - (len(intermediate_fold_data[0])*args['validationdata_split']))\n",
    "    for i in range(len(intermediate_fold_data)):\n",
    "        args['traindata'].append(intermediate_fold_data[i][:split_index])\n",
    "        args['validationdata'].append(intermediate_fold_data[i][split_index:])\n",
    "\n",
    "    print('{} sequences in train data'.format(len(args['traindata'][i])))\n",
    "    print('{} sequences in test data'.format(len(args['testdata'][i])))\n",
    "    print('{} sequences in validation data'.format(len(args['validationdata'][i])))\n",
    "\n",
    "    # split traindata into traindatasplit sets\n",
    "    if args['traindata_split'] > 1:\n",
    "        split_traindata = []\n",
    "        elems_per_split = int(round(len(args['traindata'][0])/args['traindata_split']))\n",
    "        for split in range(args['traindata_split']):\n",
    "            split_traindata_array = []\n",
    "            for i in range(len(args['traindata'])):\n",
    "                split_traindata_array.append(args['traindata'][i][split*elems_per_split:(split + 1)*elems_per_split])\n",
    "            split_traindata.append(split_traindata_array)\n",
    "        args['traindata'] = split_traindata[args['traindata_index']]\n",
    "        print('{} sequences in split train data'.format(len(args['traindata'][i])))\n",
    "\n",
    "    # duplicate traindata\n",
    "    if args['traindata_duplicate'] > 0:\n",
    "        print('{} elements before duplicating'.format(len(args['traindata'][i])))\n",
    "        args['traindata'] = regularization.DuplicateData(args['traindata_duplicate'],args['traindata'])\n",
    "        print('{} elements after duplicating'.format(len(args['traindata'][i])))\n",
    "\n",
    "    # bag traindata\n",
    "    if args['bagging']:\n",
    "        print('{} elements before bagging with putback {}'.format(len(args['traindata'][i]), args['bagging_putback']))\n",
    "        args['traindata'] = regularization.BagArray(args['bagging_size'],args['traindata'], args['bagging_putback'])\n",
    "        print('{} elements after bagging with putback {}'.format(len(args['traindata'][i]), args['bagging_putback']))\n",
    "\n",
    "    # shuffle traindata\n",
    "    if args['traindata_shuffle']:\n",
    "        args['traindata'] =  regularization.ShuffleArray(args['traindata'])\n",
    "        print('traindata shuffled')\n",
    "\n",
    "    # generate sentences from training data \n",
    "    if args['datageneration_pattern'] == DataGenerationPattern.Fit:\n",
    "        print('perform full in-memory sentence generation')\n",
    "        args['train_sentences'] = CreateSentences(args['traindata'])\n",
    "        args['validation_sentences'] = CreateSentences(args['validationdata'])\n",
    "        print('train_sentences:', len(args['train_sentences'][0]))\n",
    "        print('validation_sentences:', len(args['validation_sentences'][0]))\n",
    "    elif args['datageneration_pattern'] == DataGenerationPattern.Generator:\n",
    "        isTensorflow = args['processor'] == Processor.TPU\n",
    "        args['train_generator'] = generator.GenerateGenerator(isTensorflow, args['traindata'],args)\n",
    "        args['validation_generator'] = generator.GenerateGenerator(isTensorflow, args['validationdata'],args,shuffle=False)\n",
    "        print('created sentence generators')\n",
    "    else:\n",
    "        raise ValueError(\"unknown value for datageneration_pattern\")\n",
    "    return args\n",
    "\n",
    "def __Train_Model(args):\n",
    "    # start building input matrix\n",
    "    if args['datageneration_pattern'] == DataGenerationPattern.Fit:\n",
    "        print('Vectorization...')\n",
    "        train_matrices = args['datadefinition'].CreateMatrices(args['train_sentences'],args)\n",
    "        validation_matrices = args['datadefinition'].CreateMatrices(args['validation_sentences'],args)\n",
    "        x_train = train_matrices['X']\n",
    "        y_train = train_matrices['y_t']\n",
    "        x_val = validation_matrices['X']\n",
    "        y_val = validation_matrices['y_t']\n",
    "    elif args['datageneration_pattern'] == DataGenerationPattern.Generator:\n",
    "        print('Generators detected: Vectorization will be performed during runtime')\n",
    "    else:\n",
    "        raise ValueError(\"unknown value for datageneration_pattern\")\n",
    "\n",
    "    # build the model: \n",
    "    print('Build model...')\n",
    "    model = CreateModel(args)\n",
    "    callbacks =CreateCallbacks(args)\n",
    "\n",
    "    model.summary()\n",
    "    verbositylevel = 2\n",
    "    if args['verbose']:\n",
    "        verbositylevel = 1\n",
    "\n",
    "    # use tpu calls if tpu is defined\n",
    "    if(args['processor'] == Processor.TPU):\n",
    "        #do stuff\n",
    "        import tensorflow as tf\n",
    "        model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(args['TPU_WORKER'])))\n",
    "        args['batch_size'] = args['batch_size'] * 8 #give each tpu batch_size elements    \n",
    "        print('tpu detected: adjusting batch_size to ',  args['batch_size'])    \n",
    "    if(args['datageneration_pattern'] == DataGenerationPattern.Fit):\n",
    "        model.fit(x_train, {'time_output':y_train}, validation_data=(x_val,y_val), verbose=verbositylevel, callbacks=callbacks, shuffle=True, batch_size=args['batch_size'], epochs=args['max_epochs'])\n",
    "    elif(args['datageneration_pattern'] == DataGenerationPattern.Generator):\n",
    "        model.fit_generator(generator=args['train_generator'],\n",
    "            validation_data=args['validation_generator'],\n",
    "            verbose=verbositylevel, \n",
    "            callbacks=callbacks, shuffle=True, epochs=args['max_epochs'])\n",
    "    else:\n",
    "        raise ValueError(\"no training data found\")        \n",
    "    return model\n",
    "\n",
    "def __Evaluate_Model(args,model = None):\n",
    "    #prediction:\n",
    "    print('Load model for predictions...') \n",
    "\n",
    "    # save model file if not exists     \n",
    "    if os.path.exists(args['modelfilename']) == False:\n",
    "        model.save(args['modelfilename'])  \n",
    "        print('Model file does not exist, saving...')\n",
    "    \n",
    "    if args['save_model'] == False and model is not None:\n",
    "        # cannot load model because it's not saved, but it is supplied  \n",
    "        if args['processor'] == Processor.TPU:\n",
    "            args['processor'] = Processor.CPU # make predictions on a cpu based model\n",
    "            model.save_weights('{}_modelweights.h5'.format(args['running']))\n",
    "            import tensorflow as tf # if it runs on tpu: tensorflow\n",
    "#             tf.keras.backend.clear_session()\n",
    "            model = CreateModel(args)\n",
    "            model.load_weights('{}_modelweights.h5'.format(args['running']))\n",
    "            print('tpu detected: Model saved with last weights and converted to cpu model for cpu inference')\n",
    "        else: \n",
    "            print('Model loaded from memory')\n",
    "    elif args['save_model'] == True:\n",
    "        # model loaded from file, by recreating the model and loading weights\n",
    "        model = CreateModel(args)\n",
    "        model.load_weights(args['modelfilename'])\n",
    "        print('Model loaded from checkpoint')\n",
    "    else:\n",
    "        raise ValueError(\"no model for predictions supplied / found\")        \n",
    "    model.summary()\n",
    "\n",
    "    #evaluate\n",
    "    args['datadefinition'].MakePredictions(model,args)    \n",
    "    # configuration.Clean_Session()\n",
    "    configuration.Clean_Session()\n",
    "\n",
    "    #send results\n",
    "    if args['target_host'] != '':\n",
    "        send_results.SendResultFiles(args, [args['modelfilename'], args['testresultsfilename']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send results\n",
    "\n",
    "\n",
    "#http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file\n",
    "\n",
    "def SendResultFiles(args, files):\n",
    "    url = args['target_host']\n",
    "    for file in files:\n",
    "        try:\n",
    "            fin = open(file, 'rb')\n",
    "            files = {'file': fin}\n",
    "            headers = {'authtoken': 'thisisatoken'}\n",
    "            r = requests.post(url, files=files, headers=headers)\n",
    "            print(r.text)\n",
    "        finally:\n",
    "            fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDatadefinition(ABC): \n",
    "    \"\"\" generic implementation for the functions CreateMatrices and MakePredictions. needs to override GetDataset and GetRowstructure \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def GetDataset(self):\n",
    "        raise NotImplementedError(\"not implemented\")\n",
    "     \n",
    "    @abstractmethod\n",
    "    def GetRowstructure(self):        \n",
    "        raise NotImplementedError(\"not implemented\")\n",
    "\n",
    "    def CreateMatrices(self,sentences,args):\n",
    "        divisors = args['divisors']\n",
    "        indices = args['indices']\n",
    "        weights = args['feature_weights']\n",
    "        num_features = args['num_features']\n",
    "        rowstructure = args['rowstructure']\n",
    "        maxlen = args['maxlen']\n",
    "        verbose = args['verbose']\n",
    "\n",
    "        X = np.zeros((len(sentences[0]), maxlen, num_features), dtype=np.float32)\n",
    "        y_t = np.zeros((len(sentences[0]),1), dtype=np.float32)\n",
    "\n",
    "        for i in range(len(sentences[0])):\n",
    "            leftpad = maxlen-len(sentences[0][i])\n",
    "            \n",
    "            # train matrix\n",
    "            self.__EncodeMatrix(X,i,leftpad,rowstructure,sentences,divisors,indices,weights)\n",
    "            \n",
    "            # target matrix\n",
    "            for k, struc in enumerate(rowstructure): \n",
    "                # target variables\n",
    "                if struc['featuretype'] == FeatureType.Target:\n",
    "                    # TODO: more than 1 target\n",
    "                    y_t[i,0] = sentences[k][i][0]/divisors[k] #already offsetted\n",
    "\n",
    "            if verbose and args['datageneration_pattern'] != DataGenerationPattern.Generator:\n",
    "                sys.stdout.write(\"vectorized sequence {0} of {1}\\r\".format(i,len(sentences[0])))                    \n",
    "                sys.stdout.flush()\n",
    "        return {'X': X, 'y_t': y_t}\n",
    "\n",
    "    def MakePredictions(self,model,args):\n",
    "        #TODO\n",
    "        print('Make predictions...')\n",
    "        with open(args['testresultsfilename'], 'w', newline='') as csvfile:\n",
    "            spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            spamwriter.writerow([\"sequenceid\",\"sequencelength\",\"prefix\",\"completion\",\"prediction\",\"gt_prediction\",\"gt_planned\",\"gt_instance\",\"prefix_activities\",\"suffix_activities\"])\n",
    "            sequenceid = 0\n",
    "            print('sequences: {}'.format(len(args['testdata'][0])))    \n",
    "            for i in range(len(args['testdata'][0])):\n",
    "                sequencelength = len(args['testdata'][0][i]) - 1 #minus eol character\n",
    "                ground_truth = args['testdata'][6][i][0] + args['offsets'][6] #undo offset\n",
    "                ground_truth_processid = args['testdata'][8][i][0]\n",
    "                for prefix_size in range(1,sequencelength):   \n",
    "                    cropped_data = []\n",
    "                    for a in range(len(args['testdata'])):\n",
    "                        cropped_data.append(args['testdata'][a][i][:prefix_size])  \n",
    "                    prefix_activities = args['testdata'][0][i][:prefix_size]\n",
    "                    suffix_activities = args['testdata'][0][i][prefix_size:]\n",
    "                    if '!' in prefix_activities:\n",
    "                        break # make no prediction for this case, since this case has ended already \n",
    "\n",
    "                    # predict\n",
    "                    y = model.predict(self.__EncodePrediction(cropped_data, args['rowstructure'], args['divisors'], args['indices'],args['feature_weights'], args['num_features'], args['catvectorlen'], args['maxlen']), verbose=0)\n",
    "                    y_t = y[0][0]\n",
    "                    y_t = (y_t * args['divisors'][6]) + args['offsets'][6] #undo offset and multiply by divisor to un-normalize\n",
    "                    prediction = y_t\n",
    "\n",
    "                    #output stuff (sequence, prefix)\n",
    "                    output = []\n",
    "                    output.append(sequenceid)\n",
    "                    output.append(sequencelength)\n",
    "                    output.append(prefix_size)\n",
    "                    output.append(prefix_size / sequencelength)\n",
    "                    output.append(prediction)\n",
    "                    output.append(ground_truth)\n",
    "                    output.append(\"\")\n",
    "                    output.append(ground_truth_processid)\n",
    "                    output.append(' '.join(prefix_activities))\n",
    "                    output.append(' '.join(suffix_activities))\n",
    "                    spamwriter.writerow(output)    \n",
    "\n",
    "                    # out if an prediction was requested for a productive system\n",
    "                    #if productive_prediction:\n",
    "                    #    print('PredictedBuffer={}'.format(y_t)) \n",
    "\n",
    "                sequenceid += 1\n",
    "                if args['verbose']:\n",
    "                    print(\"finished sequence {} of {}\".format(sequenceid,len(args['testdata'][0])))\n",
    "                #end sequence loop\n",
    "        print('finished prediction')\n",
    "\n",
    "    def __EncodePrediction(self, sentence, rowstructure, divisors, indices, weights,  num_features, catvectorlen, maxlen):\n",
    "        X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
    "        leftpad = maxlen-len(sentence[0])\n",
    "        self.__EncodeMatrix(X, 0, leftpad, rowstructure, sentence, divisors, indices, weights)\n",
    "        return X\n",
    "    \n",
    "    def __EncodeMatrix(self, X, index, leftpad, rowstructure, sentences, divisors, indices, weights):\n",
    "        for j in range (len(sentences[0][index])):\n",
    "            catvectorpad = 0  \n",
    "            # train matrix\n",
    "            for k, struc in enumerate(rowstructure):                         \n",
    "                if struc['featuretype'] == FeatureType.Train:\n",
    "                    # manage strings/labels\n",
    "                    if struc['datatype'] == DataType.String:\n",
    "                        if struc['dataclass'] == DataClass.Onehot:\n",
    "                            for c in indices[\"chars\"][k]:\n",
    "                                if c == sentences[k][index][j]:\n",
    "                                    X[index, j+leftpad, catvectorpad + indices[\"chars_indices\"][k][c]] = weights[k]\n",
    "                            catvectorpad += len(indices[\"chars_indices\"][k])\n",
    "                        elif struc['dataclass'] == DataClass.Multilabel:\n",
    "                            for c in indices[\"unique_chars\"][k]:\n",
    "                                if c in sentences[k][index][j]:\n",
    "                                    X[index, j+leftpad, catvectorpad + indices[\"unique_chars_indices\"][k][c]] = weights[k]\n",
    "                            catvectorpad += len(indices[\"unique_chars_indices\"][k])\n",
    "                        else:\n",
    "                            raise NotImplementedError(\"string dataclass not implemented\")\n",
    "                    # manage numerics\n",
    "                    elif struc['datatype'] == DataType.Float:\n",
    "                        if struc['dataclass'] == DataClass.Numeric:\n",
    "                            X[index, j+leftpad, catvectorpad] = ((sentences[k][index][j]/divisors[k]) * weights[k]) \n",
    "                            catvectorpad += 1\n",
    "                        elif struc['dataclass'] == DataClass.Periodic:\n",
    "                            raise NotImplementedError(\"periodic dataclass not implemented\")\n",
    "                        else:\n",
    "                            raise NotImplementedError(\"numeric dataclass not implemented\")\n",
    "                    # special case: no dataclasses for int; treat as numeric\n",
    "                    elif struc['datatype'] == DataType.Int:                                \n",
    "                        X[index, j+leftpad, catvectorpad] = ((sentences[k][index][j]/divisors[k]) * weights[k]) \n",
    "                        catvectorpad += 1\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"datatype not implemented\")\n",
    "            X[index, j+leftpad, catvectorpad] = j + 1 #index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "# from utility.enums import DataType as dt\n",
    "# from utility.enums import DataClass as dc\n",
    "# from utility.enums import FeatureType as ft\n",
    "# from utility.enums import DataGenerationPattern, Processor, RnnType\n",
    "# from datadefinitions.generic import GenericDatadefinition\n",
    "\n",
    "class BPI2012(GenericDatadefinition):\n",
    "    def GetDataset(self):\n",
    "        return uploaded #\"datasets/bpi2012.csv\"\n",
    "\n",
    "    def GetRowstructure(self):\n",
    "        rowstructure = []\n",
    "        rowstructure.append({'datatype':DataType.String, 'columnindex':2 , 'dataclass':DataClass.Onehot, 'featuretype':FeatureType.Train, 'featureweight':1.0}) #event label (onehot) \n",
    "        rowstructure.append({'datatype':DataType.String, 'columnindex':5, 'dataclass':DataClass.Onehot, 'featuretype':FeatureType.Train, 'featureweight':1.0 }) #resource   \n",
    "        rowstructure.append({'datatype':DataType.Float, 'columnindex':3, 'dataclass':DataClass.Numeric, 'featuretype':FeatureType.Train, 'featureweight':1.0 }) #duration (always 0)\n",
    "        rowstructure.append({'datatype':DataType.Float, 'columnindex':4, 'dataclass':DataClass.Numeric, 'featuretype':FeatureType.Train, 'featureweight':1.0 }) #timestamp    \n",
    "    \n",
    "        rowstructure.append({'datatype':DataType.String, 'columnindex':6, 'dataclass':DataClass.Onehot, 'featuretype':FeatureType.Target }) #t/f\n",
    "        rowstructure.append({'datatype':DataType.String, 'columnindex':7, 'dataclass':DataClass.none, 'featuretype':FeatureType.none }) #caseid\n",
    "        return rowstructure\n",
    "\n",
    "    def CreateMatrices(self,sentences,args):\n",
    "        divisors = args['divisors']\n",
    "        indices = args['indices']\n",
    "        weights = args['feature_weights']\n",
    "        num_features = args['num_features']\n",
    "        catvectorlen = args['catvectorlen']\n",
    "        maxlen = args['maxlen']\n",
    "        verbose = args['verbose']\n",
    "\n",
    "        X = np.zeros((len(sentences[0]), maxlen, num_features), dtype=np.float32)\n",
    "        y_t = np.zeros((len(sentences[0]),1), dtype=np.float32)\n",
    "\n",
    "        for i in range (len(sentences[0])):\n",
    "            leftpad = maxlen-len(sentences[0][i])\n",
    "            for j in range (len(sentences[0][i])):      \n",
    "                catvectorpad = 0    \n",
    "                # set oh vector (event label)\n",
    "                for c in indices[\"chars\"][0]:\n",
    "                    if c == sentences[0][i][j]:\n",
    "                        X[i, j+leftpad, indices[\"chars_indices\"][0][c]] = 1\n",
    "                catvectorpad += len(indices[\"chars_indices\"][0])\n",
    "                # set oh vector (resources)\n",
    "                for c in indices[\"chars\"][1]:\n",
    "                    if c == sentences[1][i][j]:\n",
    "                        X[i, j+leftpad, catvectorpad + indices[\"chars_indices\"][1][c]] = 1\n",
    "                # set time vector\n",
    "                X[i, j+leftpad, catvectorlen] = j + 1 #index\n",
    "                X[i, j+leftpad, catvectorlen+1] = sentences[2][i][j]/divisors[2]\n",
    "                X[i, j+leftpad, catvectorlen+2] = sentences[3][i][j]/divisors[3] \n",
    "\n",
    "            if sentences[4][i][0] == \"False\":\n",
    "                y_value = 0\n",
    "            else:\n",
    "                y_value = 1\n",
    "            y_t[i,0] = y_value\n",
    "            if verbose and args['datageneration_pattern'] != DataGenerationPattern.Generator:\n",
    "                sys.stdout.write(\"vectorized sequence {0} of {1}\\r\".format(i,len(sentences[0])))\n",
    "                sys.stdout.flush()\n",
    "        return {'X': X, 'y_t': y_t}\n",
    "\n",
    "    def MakePredictions(self,model,args):\n",
    "        print('Make predictions...')\n",
    "        with open(args['testresultsfilename'], 'w', newline='') as csvfile:\n",
    "            spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            spamwriter.writerow([\"sequenceid\",\"sequencelength\",\"prefix\",\"completion\",\"prediction\",\"gt_prediction\",\"gt_planned\",\"gt_instance\",\"prefix_activities\",\"suffix_activities\"])\n",
    "            sequenceid = 0\n",
    "            print('sequences: {}'.format(len(args['testdata'][0])))    \n",
    "            for i in range(len(args['testdata'][0])):\n",
    "                sequencelength = len(args['testdata'][0][i]) - 1 #minus eol character\n",
    "                ground_truth = args['testdata'][4][i][0]\n",
    "                ground_truth_processid = args['testdata'][5][i][0]\n",
    "                for prefix_size in range(1,sequencelength):   \n",
    "                    cropped_data = []\n",
    "                    for a in range(len(args['testdata'])):\n",
    "                        cropped_data.append(args['testdata'][a][i][:prefix_size])  \n",
    "                    prefix_activities = args['testdata'][0][i][:prefix_size]\n",
    "                    suffix_activities = args['testdata'][0][i][prefix_size:]\n",
    "\n",
    "                    # predict\n",
    "                    y = model.predict(self.__EncodePrediction(cropped_data, args['divisors'], args['indices'], args['num_features'], args['catvectorlen'], args['maxlen']), verbose=0)\n",
    "                    y_t = y[0][0]\n",
    "                    prediction = y_t\n",
    "\n",
    "                    #output stuff (sequence, prefix)\n",
    "                    output = []\n",
    "                    output.append(sequenceid)\n",
    "                    output.append(sequencelength)\n",
    "                    output.append(prefix_size)\n",
    "                    output.append(prefix_size / sequencelength)\n",
    "                    output.append(prediction)                    \n",
    "                    output.append(ground_truth)\n",
    "                    output.append(0.5) # binary prediction with [0...1]\n",
    "                    output.append(ground_truth_processid)\n",
    "                    output.append(' ')\n",
    "                    output.append(' ')\n",
    "                    #output.append(' '.join(prefix_activities))\n",
    "                    #output.append(' '.join(suffix_activities))\n",
    "                    spamwriter.writerow(output)    \n",
    "\n",
    "                sequenceid += 1\n",
    "                if args['verbose']:\n",
    "                    print(\"finished sequence {} of {}\".format(sequenceid,len(args['testdata'][0])))\n",
    "                #end sequence loop\n",
    "        print('finished prediction')\n",
    "\n",
    "    def __EncodePrediction(self,sentence, divisors, indices,  num_features, catvectorlen, maxlen):\n",
    "        X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
    "        leftpad = maxlen-len(sentence[0])\n",
    "        for i in range(len(sentence[0])):\n",
    "            catvectorpad = 0 \n",
    "            # set oh vector\n",
    "            for c in indices[\"chars\"][0]:\n",
    "                if c == sentence[0][i]:\n",
    "                    X[0, i+leftpad, indices[\"chars_indices\"][0][c]] = 1\n",
    "            catvectorpad += len(indices[\"chars_indices\"][0])\n",
    "            # set ml vector\n",
    "            for c in indices[\"chars\"][1]:\n",
    "                if c == sentence[1][i]:\n",
    "                    X[0, i+leftpad, catvectorpad + indices[\"chars_indices\"][1][c]] = 1\n",
    "            X[0, i+leftpad, catvectorlen] = i + 1 #index\n",
    "            X[0, i+leftpad, catvectorlen+1] = sentence[2][i]/divisors[2] \n",
    "            X[0, i+leftpad, catvectorlen+2] = sentence[3][i]/divisors[3]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown environment detected, defaulting to gpu\n",
      "eventlog defined: bpi2012.csv\n",
      "datadefinition defined: <__main__.BPI2012 object at 0x7fc56f4181f0>\n",
      "running defined: 0\n",
      "datageneration_pattern defined: DataGenerationPattern.Fit\n",
      "bagging defined: False\n",
      "bagging_putback defined: True\n",
      "bagging_size defined: 0.8\n",
      "traindata_split defined: 1\n",
      "traindata_split_index defined: 0\n",
      "traindata_duplicate defined: 0\n",
      "traindata_shuffle defined: False\n",
      "validationdata_split defined: 0.2\n",
      "testdata_split defined: 0.3333\n",
      "max_sequencelength defined: 150000\n",
      "batch_size defined: 64\n",
      "rnntype defined: RnnType.LSTM\n",
      "neurons defined: 100\n",
      "dropout defined: 0.1\n",
      "max_epochs defined: 500\n",
      "layers defined: 2\n",
      "gradientclipvalue defined: 3\n",
      "patience_earlystopping defined: 20\n",
      "patience_reducelr defined: 20\n",
      "processor defined: Processor.GPU\n",
      "cudnn defined: True\n",
      "stateful defined: False\n",
      "save_model defined: True\n",
      "tensorboard defined: False\n",
      "verbose defined: False\n",
      "no value found for datadefinition.featureweight at index 4, setting default value\n",
      "no value found for datadefinition.featureweight at index 5, setting default value\n",
      "read 190827 rows\n",
      "sequences truncated to 150000\n",
      "offset0: null\n",
      "offset1: null\n",
      "offset2: 0\n",
      "offset3: 0\n",
      "offset4: null\n",
      "offset5: null\n",
      "divisor0: null\n",
      "divisor1: null\n",
      "divisor2: 4.770425059696304\n",
      "divisor3: 9547.828404708018\n",
      "divisor4: null\n",
      "divisor5: null\n",
      "maxlen 106 \n",
      "total chars: 23, target chars: 24\n",
      "characters:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W']\n",
      "unique characters:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W']\n",
      "total chars: 68, target chars: 69\n",
      "characters:  ['10124', '10125', '10138', '10188', '10228', '10609', '10629', '10779', '10789', '10809', '10821', '10859', '10861', '10862', '10863', '10880', '10881', '10889', '10899', '10909', '10910', '10912', '10913', '10914', '10929', '10931', '10932', '10933', '10935', '10939', '10971', '10972', '10982', '11000', '11001', '11002', '11003', '11009', '11019', '11029', '11049', '11079', '11111', '11119', '11120', '11121', '11122', '11169', '11179', '11180', '11181', '11189', '112', '11200', '11201', '11202', '11203', '11254', '11259', '11269', '11289', '11299', '11300', '11302', '11304', '11309', '11319', '11339']\n",
      "unique characters:  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "total chars: 2, target chars: 3\n",
      "characters:  ['False', 'True']\n",
      "unique characters:  ['F', 'T', 'a', 'e', 'l', 'r', 's', 'u']\n",
      "category vectors length: 91\n",
      "num features: 94\n",
      "6980 sequences in train data\n",
      "4362 sequences in test data\n",
      "1745 sequences in validation data\n",
      "perform full in-memory sentence generation\n",
      "train_sentences: 101489\n",
      "validation_sentences: 26772\n",
      "Vectorization...\n",
      "Build model...\n",
      "imported keras API for model creation\n",
      "creating stateless cudnn lstm model\n"
     ]
    },
    {
     "ename": "ConductorError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-b78208f056bc>\u001b[0m in \u001b[0;36mTrain_And_Evaluate\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__Preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__Train_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0m__Evaluate_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-b78208f056bc>\u001b[0m in \u001b[0;36m__Train_Model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCreateCallbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-44ff76de5235>\u001b[0m in \u001b[0;36mCreateModel\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating stateless cudnn lstm model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m__createCUDNN_LSTM_Stateless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_impl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-44ff76de5235>\u001b[0m in \u001b[0;36m__createCUDNN_LSTM_Stateless\u001b[0;34m(keras_impl, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#     model = keras_impl.models.Sequential()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_subclassed_network\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_base_init\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConductorError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-bf7ce916e465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# param = float(sys.argv[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# utility.run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m Train_And_Evaluate(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0meventlog\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#\"datasets/bpi2012.csv\",  # file to read in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-b78208f056bc>\u001b[0m in \u001b[0;36mTrain_And_Evaluate\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# catch all, throw one exception for parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# raise exceptions.ConductorError(ex)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mConductorError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__ReadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConductorError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "# import utility.run\n",
    "# import os\n",
    "# import sys\n",
    "# from utility.enums import DataGenerationPattern, Processor, RnnType\n",
    "\n",
    "#check for env variable for cpu/gpu environment detection\n",
    "processorType = os.environ.get(\"CONDUCTHOR_TYPE\")\n",
    "if processorType == \"cpu\":\n",
    "    print(\"cpu environment detected\")\n",
    "elif processorType == \"gpu\":\n",
    "    print(\"gpu environment detected\")\n",
    "else:\n",
    "    print(\"unknown environment detected, defaulting to gpu\")\n",
    "    processorType = \"cpu\"\n",
    "\n",
    "#import data definition\n",
    "# import datadefinitions.bpi2012 as datadef\n",
    "datadef = BPI2012()\n",
    "\n",
    "# if len(sys.argv) > 1:\n",
    "    # param = float(sys.argv[1])\n",
    "# utility.run.\n",
    "Train_And_Evaluate(\n",
    "    #data\n",
    "    eventlog= uploaded, #\"datasets/bpi2012.csv\",  # file to read in\n",
    "    datadefinition=datadef,          # the data / matrix definitions\n",
    "    running=0,                       # iterable / suffix\n",
    "    datageneration_pattern = DataGenerationPattern.Fit, # Fit: uses the classical approach and loads everything into memory; Generator: uses the python generator pattern\n",
    "    #regularization/dataset manipulation\n",
    "    bagging=False,                   # perform bagging? \n",
    "    bagging_putback=True,            # (if bagging) elements get put back and can be drawn again \n",
    "    bagging_size=0.8,                # (if bagging) the split to bag train data\n",
    "    traindata_split=1,               # split the train data into X pieces; default 1 (no split)\n",
    "    traindata_split_index=0,         # the index of the split to pick; default 0\n",
    "    traindata_duplicate=0,           # duplicates part of the train data to (virtually) inflate it; default 0 (no duplicates)\n",
    "    traindata_shuffle=False,         # shuffle training data\n",
    "    validationdata_split = 0.2,      # Validation data split from traindata (default 0.2)\n",
    "    testdata_split = 0.3333,         # test data split from input data. takes data from the bottom of the dataset (default 0.333)\n",
    "    max_sequencelength=150000,       # maximum allowed sequence length\n",
    "    #framework/ann specifics\n",
    "    batch_size=64,                   # batch size (set to 1 for stateful)\n",
    "    rnntype=RnnType.LSTM,            # type of rnn cell to use: lstm, gru or rnn (vanilla)\n",
    "    neurons=100,                     # neurons per layer\n",
    "    dropout=0.1,                     # dropout per layer (not applicable to CuDNN)\n",
    "    max_epochs = 500,                # maximum amount of epochs to run (uses early_stopping)\n",
    "    layers=2,                        # layers for the rnn\n",
    "    gradientclipvalue=3,             # value to clip the gradient to if it exceeds it\n",
    "    patience_earlystopping=20,       # patience for early stopping\n",
    "    patience_reducelr=20,            # patience for lr reduction\n",
    "    processor=Processor.GPU,         # processor, cpu, gpu or tpu (gpu uses CUDNN based algorithms for lstm and gru if cudnn is set to true)\n",
    "    cudnn=True,                      # (if GPU) utilizes special nVidia-CuDNN LSTM and GRU implementations\n",
    "    stateful=False,                  # stateful or stateless model; default False\n",
    "    #debug\n",
    "    save_model=True,                # saves the model file each checkpoint (set to False for TPU usage)\n",
    "    tensorboard=False,               # outputs tensorboard compatible eventlog into ./graph folder for visualization\n",
    "    verbose=False)                  # prints out a lot of progress reports. do NOT use with cluster learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
